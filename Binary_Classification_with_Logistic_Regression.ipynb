{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Binary Classification with Logistic Regression\n",
    "\n",
    "Â© Explore Data Science Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "In this train you will learn how to:\n",
    "\n",
    "- Understand binary classification;\n",
    "- Understand logistic regression;\n",
    "- Implement a logistic regression model in `sklearn`.\n",
    "\n",
    "## Outline\n",
    "\n",
    "This train is structured as follows:\n",
    "\n",
    "- Introduction to binary classification;\n",
    "- Introduction to logistic regression;\n",
    "- Implementing a logistic regression model in `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "\n",
    "### Quantitative vs. qualitative variables\n",
    "\n",
    "In this course we have explored a variety of ways which we can use to predict a response variable _Y_ when that response is quantitative, or numerical, in nature. The numerical variables we often predict typically exist on a continuous scale, and we can therefore fit things like a linear regression which, for increasing values of X, there's appropriate increasing/decreasing values of Y. An example of linear regression is shown below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-linear-reg.png\" alt=\"sketch-linear-reg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, however, the response variable Y in our dataset will not be continuous - it will be categorical(qualitative). The prediction task in this case is known as _classification,_ because we are trying to _classify_ into which category an observation belongs.\n",
    "\n",
    "The simplest version of this scenario is known as _binary classification_. Binary refers to the fact that there are two categories. It is normally intuitive to encode the two classes in a binary classification task as zeroes and ones.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-categorical-y.png\" alt=\"sketch-categorical-y\" style=\"width: 300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our response variable encoded as zeroes and ones, we are able to plot some of the data points for a predictor variable X, and see the peculiar problem we are faced with: the response values lie along two distinct horizontal lines. What's more, they only ever take on values of 0 and 1! Clearly no good for a linear regression, which is only ever monotonically increasing or decreasing.\n",
    "\n",
    "Looking at the values for Y which lie along an example linear regression line, we can see that for very large X, we will be getting values for Y which are greater than 1, and as X decreases, we may even get negative Y values - far from the zeroes and ones we need.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-linear-classification.png\" alt=\"sketch-linear-classification\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to this problem is known as *logistic regression*, so-called because it makes use of a common S-shaped curve known as the logistic function. This curve is commonly known as a _sigmoid_. It solves the problem for the following reasons:\n",
    "\n",
    "- It squeezes the range of output values to exist only between 0 and 1.\n",
    "- It has a point of inflection, which can be used to separate the feature space into two distinct areas (one for each class).\n",
    "- It has shallow gradients at both its top and bottom, which can be mapped to zeroes or ones respectively with little ambiguity.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-logistic-regression.png\" alt=\"sketch-logistic-regression\" style=\"width: 600px;\"/>\n",
    "\n",
    "The name is a little confusing because it contains the word _regression_. That's because we are effectively doing a linear regression, but then squeezing that linear regression vertically down into the S-shape sigmoid curve, so that it exists between 0 and 1. Rest assured - this is definitely a **classification model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model produces output values for the response variable which again lie on the curve itself. However, unlike regression, where the output value was also the prediction value, in logistic regression the output value is a _probability_. The value along the curve (which, remember, is always between zero and one), is equal to the probability of the observation belonging to class 1 in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Logistic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know that the output value of a logistic regression model refers to the probability that the observation in question belongs to class 1. The output values all fall between 0 and 1, which is all very well. But at what threshold value do we decide that a probability is too low to be assigned to class 1? Usually, we pick 0.5. That is:\n",
    "\n",
    "- Values greater than or equal to 0.5 are assigned to class 1; and\n",
    "- Values less than 0.5 are assigned to class 0.\n",
    "\n",
    "This output needs to hold for all values of X. In other words, regardless of the value of X, we need the output to be a value between 0 and 1. The function that takes care of all this is defined as follows:\n",
    "\n",
    "$$P(X) = \\displaystyle \\frac{e^{\\beta_0 + \\beta_1 X}}{1+e^{\\beta_0 + \\beta_1 X}}$$\n",
    "\n",
    "where $P(X)$ is the probability of X belonging to class 1, and $\\beta_0$ and $\\beta_1$ are the intercept and regression coefficient respectively, just like in a linear regression model. After a bit of manipulation we arrive at:\n",
    "\n",
    "\\begin{align}\n",
    "1 - P(X) &= \\displaystyle \\frac{1}{1+e^{\\beta_0 + \\beta_1 X}} \\\\\n",
    "\\therefore \\log \\left( \\frac{P(X)}{1-P(X)} \\right) &= {\\beta_0 + \\beta_1 X}\n",
    "\\end{align}\n",
    "\n",
    "The term on the left is known as the **log odds ratio**. Without the log sign in front of it, it is known simply as the odds ratio. While $P(X)$ is bounded between 0 and 1, the odds ratio is bounded between 0 and $\\infty$. \n",
    "\n",
    "_Note: if you're still having trouble understanding the concept of log odds ratios, please checkout videos in the additional links section._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Classification\n",
    "\n",
    "There will clearly be cases when we have more than two categories in our labels column. Perhaps we are classifying the colour of cars into one of: `[red, green, blue]`.\n",
    "\n",
    "As we have seen, the logistic regression algorithm is effective for those situations with no more than two classes. There is, however, a neat way to combine logistic regression models for multi-class classification in what is known as the one-vs-rest approach (or OvR).\n",
    "\n",
    "In the OvR case, a separate logistic regression model is trained for each label that the response variable takes on. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-one-vs-rest.png\" alt=\"sketch-one-vs-rest\" style=\"width: 600px;\"/>\n",
    "\n",
    "Fortunately, `sklearn` makes it very simple to implement this multi-class extension of the logistic regression algorithm. We'll include a note on it in this train and the actual code for it later on in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linearly Separable Data Points\n",
    "\n",
    "One last thing to note here before we move onto implementation. Logistic regression models are effective only for those cases in which we have clearly linearly separable data. That is to say, a straight line or hyperplane can be placed somewhere amongst the datapoints that definitively separates them.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-non-separable.png\" alt=\"sketch-non-separable\" style=\"width: 600px;\"/>\n",
    "\n",
    "In those cases that our data is not linearly separable, we will need to try out other algorithms (perhaps linear discriminant analysis, or a support vector classifier). Later on in the course, we will explore many more models to carry out more complex classification tasks. Stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Logistic Regression Model\n",
    "\n",
    "In this train we will build a classification model to predict whether the policholder of some insurance product will claim from their insurance within the upcoming year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>steps</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>insurance_claim</th>\n",
       "      <th>claim_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>3009</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>yes</td>\n",
       "      <td>16884.9240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>3008</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>yes</td>\n",
       "      <td>1725.5523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3009</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>10009</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>8010</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>yes</td>\n",
       "      <td>3866.8552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  steps  children smoker     region insurance_claim  \\\n",
       "0   19  female  27.900   3009         0    yes  southwest             yes   \n",
       "1   18    male  33.770   3008         1     no  southeast             yes   \n",
       "2   28    male  33.000   3009         3     no  southeast              no   \n",
       "3   33    male  22.705  10009         0     no  northwest              no   \n",
       "4   32    male  28.880   8010         0     no  northwest             yes   \n",
       "\n",
       "   claim_amount  \n",
       "0    16884.9240  \n",
       "1     1725.5523  \n",
       "2        0.0000  \n",
       "3        0.0000  \n",
       "4     3866.8552  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data in and view first few entries\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint/claims_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by preparing the data to be used in the logistic regression algorithm, this involves:\n",
    "\n",
    "- Splitting the data into features and labels;\n",
    "- Transforming the categorical features (create dummy variables);\n",
    "- Splitting the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "y = df['insurance_claim']\n",
    "\n",
    "# features\n",
    "X = df.drop('insurance_claim', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the Features\n",
    "X_transformed = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready. Let's train the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the LogisticRegression module from `sklearn.linear_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of the `LogisticRegression()` object using the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `fit()` method to train the model.\n",
    "\n",
    "**Pro-tip**: in the multi-class case we referred to above, the `LogisticRegression` instance takes a simple argument which enables it to be used for 2+ classes: `multi_class='ovr'`. We'll revisit this later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can extract the parameters. The parameters consist of the intercept and the coefficients related to the features. These parameters can be used to predict future claims given the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intercept\n",
    "\n",
    "The interpretation of the parameters of the logistic model is not quite the same as for a linear regression model. \n",
    "\n",
    "In binary classification, the class with value 1 is known as the _reference class_. Let's explore.\n",
    "\n",
    "The intercept, $\\beta_0$, is interpreted as the **log odds** ratio of an observation being in the reference class when all other predictor variables are equal to zero.\n",
    "\n",
    "We can exponentiate this value, in other words raise the natural number $e$ to this value, to convert it to a typical **odds** ratio. In other words:\n",
    "\n",
    "$$Odds = e^{\\beta_0}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.641814282153384e-05"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficients\n",
    "\n",
    "For binary categorical variables, like `smoker` and `sex`, the coefficient is interpreted as the **log odds** ratio between the class implied by a zero for the variable (i.e. non-smoker), and the class implied by a one for the variable (i.e. smoker).\n",
    "\n",
    "For continuous variables, the coefficient is interpreted as the expected change in the log odds for a one-unit increase in the variable.\n",
    "\n",
    "Again, we can arrive at the usual odds value by exponentiating the coefficient:\n",
    "\n",
    "$$Odds = e^{\\beta_1}$$\n",
    "\n",
    "Effectively, each coefficient is a measure of the change in the log odds of belonging to the reference class for one-unit changes in the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>age</td>\n",
       "      <td>-0.002561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bmi</td>\n",
       "      <td>-0.001819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>steps</td>\n",
       "      <td>-0.004556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>children</td>\n",
       "      <td>-0.000208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>claim_amount</td>\n",
       "      <td>0.038041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sex_male</td>\n",
       "      <td>-0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>smoker_yes</td>\n",
       "      <td>-0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region_northwest</td>\n",
       "      <td>-0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region_southeast</td>\n",
       "      <td>-0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region_southwest</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Coefficient\n",
       "age                 -0.002561\n",
       "bmi                 -0.001819\n",
       "steps               -0.004556\n",
       "children            -0.000208\n",
       "claim_amount         0.038041\n",
       "sex_male            -0.000007\n",
       "smoker_yes          -0.000013\n",
       "region_northwest    -0.000023\n",
       "region_southeast    -0.000014\n",
       "region_southwest     0.000005"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_df = pd.DataFrame(lr.coef_.T, X_transformed.columns, columns=['Coefficient'])\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your own time: What can you infer from the coefficients above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Time\n",
    "\n",
    "Next we will use the `predict` method to obtain predictions from our test data observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some predictions from our model, we are in a position to assess the performance of the model. In regression, we were concerned with _error rate_ , or the average closeness of our predictions to the ground truth values.\n",
    "\n",
    "In classification, we are concerned with _accuracy_ , or more broadly, how many of the observations did we correctly assign to the two classes, zero and one.\n",
    "\n",
    "The metrics and methods used to measure classification accuracy are extensive, and we'll dive into them in an upcoming train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages & Disadvantages of Logistic Regression\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- Convenient probability scores for observations (probability of each outcome is transformed into a classification);\n",
    "- Not a major issue if there is collinearity among features (much worse with linear regression).\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "- Can overfit when data is unbalanced (i.e.: we have far more observations in one class than the other);\n",
    "- Doesn't handle large number of categorical variables well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train we have seen or been introduced to:\n",
    "- Classification, and specifically binary (or two-class) classification;\n",
    "- Logistic regression, its similarities and differences compared to linear regression;\n",
    "- As well its effectiveness and simplicity in binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Links to additional resources to help with the understanding of concepts presented in the train: \n",
    "\n",
    "- [Statquest video on odds and log odds](https://www.youtube.com/watch?v=ARfXDSkQf1Y)\n",
    "- [Statquest video on odds and log odds ratios](https://www.youtube.com/watch?v=8nm0G-1uJzA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
